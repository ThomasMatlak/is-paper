%!TEX root = ../username.tex
\chapter{Background} \label{bg}

Computer Music is a field of computing focused on the creation of music, either as a tool to assist composers or by using the computer to create its own music without human intervention.
The field includes the development of programs such as digital audio workstations, which provide a library of sounds for the composer to use and an interface to create music; electronic instruments; and computer generated music.
This paper focuses on a subset computer generated music: algorithmic creation of melodies.

The paper is organized as follows:
the rest of the first chapter contains background information on musical, mathematical, and computational concepts necessary to fully comprehend the project,
the second chapter discusses some of the mathematics related to what makes music pleasing,
the third chapter discusses the use of Markov chains to generate melodies,
the fourth chapter discusses the use of genetic algorithms in melody generation,
and the fifth chapter discusses the software used to implement the concepts in chapters \ref{markov} and \ref{ga}.
Finally, the last chapter discusses the results obtained from the project, including some examples of music produced by the software.

\section{Musical Terminology and Notation} \label{bg:musicTerminology}

This paper contains the some musical terminology which not all readers may be familiar with.
This section contains definitions of musical terms that are important to fully understand the rest of the paper.
It is intended as a reference for readers unfamiliar with the terms defined here, so they are provided in alphabetical order, rather than in a guided exploration of the fundamental concepts of music.

\textbf{Beat}: The beat is the fundamental unit of rhythm.
What a beat looks like is context dependent.
For example, in one piece the beat may be the quarter note, in a second piece it may be a dotted quarter note, in a third piece it might be the half note.
A tempo (speed) is generally provided to determine how long each beat should last.
This may take the form of a specific number of beats per minute or a more general term, such as \textit{Allegro} for fast, or \textit{Largo} for slow.
The beat can be multiplied or divided into smaller parts to create different rhythms.

\textbf{Chord}: A chord consists of several notes played as a single unit.
Most commonly, three or more notes are played simultaneously, though two notes may also constitute a chord.
Common chords are based on the major and minor scales, with variations depending on the order of the notes from lowest to highest.

\textbf{Chord tone}: A chord tone is a note contained in a chord.
For example, C, E, and G are chord tones of a C Major chord.

\textbf{Harmony}: Harmony consists of all the notes played at the same time as the melody that are not part of the melody.
In Western music, these harmonies generally follow a progression of chords based on the key that enhance the melody.

\textbf{Key}: The key of a piece of music determines the chords that appear, as well as the scale the music generally follows.
The name of a key takes the form of <Pitch> <Modifier>, where the pitch is one of fifteen pitches, and the modifier specifies the mode of the key, most ofter Major or Minor.
The pitch indicates the starting pitch of the scale.
Key is notated by the key signature on the music staff, a set of sharps or flats on specific lines and spaces.

\textbf{Measure}: In music, a measure is a collection of notes grouped together in order to give a piece some structure.
Measures within a piece generally contain the same number of beats.

\textbf{Melody}: The melody is the most prominent part, often played in the highest voice.
It is the recognizable tune that generally defines a piece of music.

\textbf{MIDI}: (Musical Instrument Digital Interface) This is a standard that defines a digital interface, communication protocol, and electrical connectors that allows computers, electronic instruments, and other audio devices to communicate.

\textbf{Non-chord tone}: A non-chord tone is a note that appears while a chord is played but is not part of the chord.
Non-chord tones may be used to insert dissonance and drive a piece of music toward resolution.
For example, F is a non-chord tone when played over a C Major chord.

\textbf{Note}: A note is a pitch coupled with a duration.
The pitch is given as a pitch class and the octave to which the note belongs.
For example, C4 refers to the fourth C to appear from the left on a standard $88$ key piano.
When discussing notes we often ignore the duration and only consider the pitch.

\textbf{Note length/rhythm}: Durations of notes used to indicate how much of each beat a note should occupy and are divided as follows:
The quarter note (\quarternote) is often the most basic division of the beat, at one quarter note per beat.
The eight note (\eighthnote) is half the duration of the quarter note, while the half note (\halfnote) is double the duration of the quarter note, and the whole note (\fullnote) is four times the duration.
The names for these note lengths come from how many of that type of note can fit in a common time measure (which lasts for one whole note, or more often four quarter notes).
These durations continue in ether direction, so there exist sixteenth and thirty-second notes, as well as double whole notes.
Dots can also be applied to notes to add half the duration of the base note.
For example, a dotted quarter note (\quarternote.) takes the same amount of time as three consecutive eighth notes (\eighthnote \eighthnote \eighthnote).

\textbf{Pitch classes}: When two notes have the same letter, but are not necessarily in the same octave, they have the same pitch class.
Pitch classes divide the octave into distinct parts and relate to the frequencies of the pitches.
When discussing the harmonic structure of music, notes of the same pitch class are equivalent, so the octaves in which notes appear do not change the harmonic structure.
Along with the seven basic pitch classes (A, B, C, D, E, F, and G), these may also appear with modifiers such as a sharp ($\sharp$) or flat ($\flat$) symbol, indicating the pitch should be raised or lowered.
In total there are twelve distinct pitch classes.

\textbf{Scale}: A scale is a way of choosing which notes to include in a piece and is closely related to the concept of a key.
Scales begin on one pitch class, and ascend one octave before repeating.
Different modes have different feelings.
For example, the major mode (Ionian) sounds more bright and happy, while the minor modes (Aeolian, Dorian, and Phrygian) sound more dark and sad.

\textbf{Tonic}: The tonic pitch is the base note of a scale.
For example, in the C Major scale, C is the tonic.

\section{Writing Melodies} \label{bg:writingmelodies}

In the tradition of Western music, there exist some general guidelines for creating pleasing melodies, a small sample of which follow.
Transitions between notes should primarily occur in a stepwise manner, and one should avoid leaps of more than an octave.
A piece of music as a whole should follow a chord progression, or set of recognizable chords between which a piece moves.
If we use the seventh note (also called the \textit{leading tone}) in the scale, the next note should be the tonic of the scale.
These are not all of the rules for a good melody, and some of the rules are rather broad, but they provide a decent starting point to quickly identify good and bad melodies.

Let us examine a simple example of writing our own music.
Perhaps the most common chord progression is I-V-IV-vi-I, which means the first chord is based upon the first note of the scale of the key of the piece, the next chord is based upon the fifth note in the scale, the next upon the fourth note, the next upon the sixth, and finally the final chord is based upon the first note in the scale.
If we are in the key of C Major, we might start by simply making our melody C G F A C, as in Figure \ref{fig:basic_melody}, but this is rather boring, does not move in a stepwise manner, and breaks some more advanced part writing rules.
Rather, we might rearrange the order in which notes appear in each chord to produce more interesting melodic and harmonic structure, as in Figure \ref{fig:better_melody}.
In this version, we use the same chords, but the melody moves primarily stepwise and we do not break any of the more advanced part writing rules.

\begin{figure}[h]
	\centering
	\includegraphics{figures/boring_melody.pdf}
	\caption{A simple example of a bad melody.}
	\label{fig:basic_melody}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics{figures/better_melody.pdf}
	\caption{A simple example of a good melody.}
	\label{fig:better_melody}
\end{figure}

\section[Markov Chains]{Markov Chains} \label{bg:markov}

Intuitively, we may think of a Markov chain as a machine that accepts some former state or states of a system and produces the next state in the system.
The decision of what the next state should be is based on probabilities of transitioning to different states from the current state of the system.

\subsection{Definition} \label{bg:markov:definitions}

More formally, a \textit{Markov chain} is a type of discrete-time stochastic process, which means a Markov chain is a sequence of random variables $\boldsymbol{X} = \{X_{n} | n \in I\}$ for some index set $I$.
Additionally, Markov chains have the special property that they depend only on the immediate past state(s).
That is, for a first-order Markov chain at time $t$, $$P(X_{t} = j \mid X_{0} = i_{0}, \ldots, X_{t - 1} = i_{i - 1}) = P(X_{t} = j \mid X_{t - 1} = i_{t - 1})$$ for a particular possible outcome $j$ of $X_{t}$ \cite{nierhaus_algorithmic_2009}.

This idea can also extend to higher-order Markov chains.
A higher-order Markov process considers more than the single most recent state to determine the next state.
An $n$th order Markov chain uses the previous $n$ states as the input to find the next state.

\subsection{Representations} \label{bg:markov:representations}

In the case when $n = 1$, we can think of a Markov chain as a directed graph, where each state is a node, each edge is a transition between states, and the probabilities of transitioning between states are represented by the edge weights.
See Figure \ref{fig:markovGraph} for a visual representation of this idea.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{figures/markovGraph.pdf}
	\caption[A Markov chain represented as a graph.]{A Markov chain represented as a graph. Arrows between nodes represent transitions between nodes.}
	\label{fig:markovGraph}
\end{figure}

When implementing a Markov chain in code, however, it is perhaps easier to represent it as an $(n + 1)$-dimensional array, where $n$ is the order of the Markov chain.
We call this $(n + 1)$-dimensional array the \textit{transition matrix}.
It contains the probabilities of transitioning from one state to another.
See Figure \ref{fig:markovMatrix} for an example of the same Markov chain as in Figure \ref{fig:markovGraph} in matrix form.
Note that the matrix representation is essentially an \textit{adjacency matrix} of the graph, where edge weights are the probabilities of transitioning between nodes.

\begin{figure}[h]
	\centering
	\begin{tabular}{c | c c c}
		& $A_{1}$ & $B_{1}$ & $C_{1}$\\
		\hline
		$A_{0}$ & $0.3$ & $0.5$ & $0.2$\\
		$B_{0}$ & $0.6$ & $0.2$ & $0.2$\\
		$C_{0}$ & $0.4$ & $0.1$ & $0.5$
	\end{tabular}
	\caption[A Markov chain represented as a transition matrix.]{A Markov chain represented as a transition matrix. Rows represent transitions from the labeling node to the labeling node of each column.}
	\label{fig:markovMatrix}
\end{figure}

\subsection{Limitations} \label{bg:markov:limitations}

A major limitation of Markov chains is their inability to generate truly novel output.
In order for some state to appear, a transition to that state from the previous state must appear in the \textit{source material}, the set of data from which the transition probabilities come.
That is, no truly novel transitions may appear; all transitions that appear in the output of the Markov chain must have appeared somewhere before.
Additionally, lower-order chains may produce nonsensical output, whereas a chain of sufficiently high order will exactly copy the source material.
Another limitation is that the process may get stuck in a ``local loop''.
This may happen when the chain proceeds to a state which only transitions to itself or transitions to a set of states that only transition to each other.

See Chapter \ref{markov} for more information on how Markov chains are used in this project.

\section{Artificial Neural Networks} \label{bg:nn}

\textit{Artificial neural networks} (ANNs) are a computing construct inspired by biological brains that contain a collection of connected \textit{neurons}, which we also call \textit{nodes}.
An ANN consists of a layer of input nodes, a layer of output nodes, and zero or more layers of nodes in between the input and output layers called hidden layers.
A neuron accepts one or more inputs and performs a weighted summation of the neuron's inputs and a constant input (\textit{bias}) to produce its output.
The \textit{input layer} consists of nodes that accept numerical inputs from some outside source.
The \textit{output layer} consists of nodes that expose the weighted sum of their inputs to outside sources.
\textit{Hidden layers} of nodes accept numeric inputs from the previous layer and produce numeric outputs that pass to following layers.
The previous layer can be either the input layer or another hidden layer and the following layer can be either another hidden layer or the output layer.
Hidden layers generally all contain the same number of nodes, though this is not necessary.
In general, the output from each node in a given layer is passed to every node in the following layer, and each node receives the output from every node in the previous layer as input.
See Figure \ref{fig:ann} for a visual of a simple ANN.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{figures/ann.pdf}
	\caption{A simple ANN with an input layer, one hidden layer, and an output layer.}
	\label{fig:ann}
\end{figure}

To constrain the output of the neurons, we use an \textit{activation function} to provide upper and lower bounds for the output and to force these values towards those new bounds.
Common activation functions include the identity, sigmoid (or logistic), binary step, and arctangent functions.
The identity function preserves the computed value as is, without any constraint on its bounds.
The binary step function forces the computed value to either zero or one, while the sigmoid function merely bounds the computed value to between zero and one.
The arctangent function bounds the output between $\frac{-\pi}{2}$ and $\frac{\pi}{2}$, and provides a faster ramp up from the minimum value to the maximum value.
See Figure \ref{fig:activation_functions} for some visualizations of these activation functions.

\begin{figure}
	\centering
	\includegraphics{figures/activation_functions.pdf}
	\caption{Some common activation functions. Clockwise from the top left: identity, binary step, arctangent, sigmoid.}
	\label{fig:activation_functions}
\end{figure}

Each activation function has a different case where we might want to use it.
The binary step function works well when we want a binary classification, such as in the output layer: does the input belong to a particular category?
The sigmoid and arctangent functions both constrain the output to a range of real numbers, but with different bounds and at different rates of convergence.
One possible use case for this type of function is to provide the percent certainty that an input belongs to some category.
We use the identity function when we do not want to alter the output in any way.

In order for an ANN to be useful, it must go through training to set the weight for each node and accurately predict data.
The most common algorithm to train ANNs is the \textit{backpropogration} algorithm.
In this, the weights are randomly initialized, then inputs are passed throught the network and the output is compared to the expected output.
Then, the error is used to determine the magnitude of the change to the weights.

For time series data, normal ANNs are not suitable because they consider only a single input at once.
Instead, we can use \textit{recurrent neural network} (RNN) setups, which consider the results from previous inputs when evaluating the current input.
In particular, we utilize a long-short term artificial neural network, which stores information about previous input until it determines that information is no longer relevant, at which point it is forgotten.

\section{Genetic Algorithms} \label{bg:ga}

A \textit{genetic algorithm} (GA) is an iterative process that takes an initial population of individuals and remixes and mutates that population to produce a new set of individuals to become the initial population of the next generation.
This new set is produced by performing various operations on the initial population, then using a fitness function to choose the best performing individuals.
The idea is that over the course of many generations, fitness tends to increase, as only the best performing individuals are allowed to survive to the next generation.

An essential operation for GAs is \textit{crossover}, that is, splicing individuals together to produce new sequences.
Other operations can also be defined to operate on or more individuals.
After creating the new candidate population, we introduce mutations to keep the population from stagnating.
These mutations make random changes to the candidate population.
For example, in a binary string, a $0$ may get flipped to a $1$, which may be a small change, but it could improve the fitness of the individual.

To choose the new initial population, we define a fitness function which measures how close an individual is to the desired output.
If some individuals reach a certain level of fitness, they are the output of the genetic algorithm.
Otherwise, the best performing individuals become the initial population of the next generation.
This process continues until some individuals perform well enough, a maximum number of generations is reached, or fitnesses stagnate.

As a simple example, consider a population of four random 4-bit bit strings.
Our goal is to produce a bit string consisting of all $1$s.
As our fitness function, let each $1$ in the bit string add $0.25$ to the fitness, so a bit string of all $1$s has a fitness of $1.0$.
Let the initial population contain the bit strings $0100$, $1001$, $0010$, and $1100$, which have fitness values of $0.25$, $0.5$, $0.25$, and $0.5$, respectively.
We put these individuals into the pool of candidate strings.
Next, we can randomly select some of these strings to crossover.
Consider the first and second bit strings: $0100$ and $1001$.
For the sake of simplicity, let us perform crossover at the middle.
This gives us the candidates $0101$ and $1000$.
Next, consider the third and fourth bit strings: $0010$ and $1100$.
Again, to keep the example simple, let us perform crossover at the middle.
This gives us two more candidate strings, $1110$ and $0000$.
At this point, the candidate population is $0100$, $1001$, $0010$, $1100$, $0101$, $1000$, $1110$, and $0000$.
Suppose no mutations are chosen for this generation.

Choosing the fittest four of these, we get $1110$, $1001$, $1100$, and $0101$.
Now perform crossover of the second and third bit strings, again at the middle.
This gives us $1000$ and $1101$.
At this point, the candidate population consists of $1110$, $1001$, $1100$, $0101$, $1000$, and $1101$.
Let us preform some mutations this time.
Suppose we randomly choose the first and fifth strings to mutate, each at position $4$.
This means we flip the bits of these strings at the fourth position.
Now the candidate population is $1111$, $1001$, $1100$, $0101$, $1001$, and $1101$.
We have a bit string consisting entirely of $1$s, so our goal has been accomplished, and we may exit the process.

See Chapter \ref{ga} for more information on how genetic algorithms are used in this project.
